#!/usr/bin/env python3
# data/data_manager.py - Data Management Module (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á)
# ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô

import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from utils.config_manager import get_config
from utils.logger import get_logger

class DataManager:
    """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î"""
    
    def __init__(self):
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô DataManager"""
        self.config = get_config()
        self.logger = get_logger()
        self.data_cache = {}
        
    def load_historical_data(self, symbol: str, data_dir: str = "data") -> Optional[pd.DataFrame]:
        """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á)"""
        try:
            # ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏•‡∏≠‡∏á (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç)
            possible_files = [
                f"{symbol}_yfinance_daily.csv",           # ‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å
                f"{symbol}_yfinance_daily_backup.csv",    # ‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏£‡∏≠‡∏á
                f"{symbol}_daily_fixed.csv",
                f"{symbol}_real.csv",
                f"{symbol}.csv"
            ]
            
            for filename in possible_files:
                filepath = os.path.join(data_dir, filename)
                if os.path.exists(filepath):
                    self.logger.info(f"Loading {symbol} from {filename}")
                    
                    try:
                        # ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV
                        df = pd.read_csv(filepath)
                        
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                        df = self._process_historical_data(df, symbol, filename)
                        
                        if len(df) > 50:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 50 ‡πÅ‡∏ñ‡∏ß
                            self.logger.success(f"Loaded {symbol}: {len(df)} rows from {filename}")
                            return df
                        else:
                            self.logger.warning(f"Insufficient data for {symbol} in {filename}: {len(df)} rows")
                            continue
                            
                    except Exception as e:
                        self.logger.warning(f"Failed to load {filename}: {e}")
                        continue
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á
            self.logger.warning(f"No historical data found for {symbol}, generating synthetic data")
            return self.generate_synthetic_data(symbol, 1000)
            
        except Exception as e:
            self.logger.error(f"Error loading historical data for {symbol}: {e}")
            return self.generate_synthetic_data(symbol, 1000)
    
    def _process_historical_data(self, df: pd.DataFrame, symbol: str, filename: str) -> pd.DataFrame:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö)"""
        try:
            processed_df = df.copy()
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å
            processed_df.columns = [col.lower().strip() for col in processed_df.columns]
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå
            if 'backup' in filename:
                # ‡πÑ‡∏ü‡∏•‡πå backup ‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á (Price, Close, High, Low, Open, Volume)
                self.logger.info(f"Processing backup format for {symbol}")
                processed_df = self._process_backup_format(processed_df)
            else:
                # ‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏Å‡∏ï‡∏¥ (Date, Open, High, Low, Close, Volume)
                self.logger.info(f"Processing standard format for {symbol}")
                processed_df = self._process_standard_format(processed_df)
            
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ timestamp
            processed_df = self._process_timestamp(processed_df)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå OHLCV
            processed_df = self._process_ohlcv_columns(processed_df)
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå symbol
            processed_df['symbol'] = symbol
            
            # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ NaN
            initial_rows = len(processed_df)
            processed_df = processed_df.dropna(subset=['open', 'high', 'low', 'close', 'volume'])
            final_rows = len(processed_df)
            
            if initial_rows != final_rows:
                self.logger.info(f"Removed {initial_rows - final_rows} rows with NaN values")
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏° timestamp
            processed_df = processed_df.sort_values('timestamp').reset_index(drop=True)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• OHLC
            processed_df = self._validate_ohlc_data(processed_df)
            
            return processed_df
            
        except Exception as e:
            self.logger.error(f"Error processing historical data for {symbol}: {e}")
            raise
    
    def _process_backup_format(self, df: pd.DataFrame) -> pd.DataFrame:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå backup format"""
        try:
            # ‡πÑ‡∏ü‡∏•‡πå backup: Price, Close, High, Low, Open, Volume
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
            column_mapping = {
                'price': 'close',  # ‡πÉ‡∏ä‡πâ Price ‡πÄ‡∏õ‡πá‡∏ô Close ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Close
            }
            
            # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
            for old_name, new_name in column_mapping.items():
                if old_name in df.columns and new_name not in df.columns:
                    df[new_name] = df[old_name]
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
            numeric_columns = ['open', 'high', 'low', 'close', 'volume']
            for col in numeric_columns:
                if col in df.columns:
                    # ‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å string ‡πÄ‡∏õ‡πá‡∏ô numeric
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            return df
            
        except Exception as e:
            self.logger.error(f"Error processing backup format: {e}")
            raise
    
    def _process_standard_format(self, df: pd.DataFrame) -> pd.DataFrame:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå standard format"""
        try:
            # ‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏Å‡∏ï‡∏¥: Date, Open, High, Low, Close, Volume
            # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
            
            numeric_columns = ['open', 'high', 'low', 'close', 'volume']
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            return df
            
        except Exception as e:
            self.logger.error(f"Error processing standard format: {e}")
            raise
    
    def _process_timestamp(self, df: pd.DataFrame) -> pd.DataFrame:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• timestamp"""
        try:
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå timestamp/date
            timestamp_columns = ['date', 'timestamp', 'time', 'datetime']
            timestamp_column = None
            
            for col in timestamp_columns:
                if col in df.columns:
                    timestamp_column = col
                    break
            
            if timestamp_column:
                # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô datetime
                df['timestamp'] = pd.to_datetime(df[timestamp_column], errors='coerce')
                
                # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà 'timestamp'
                if timestamp_column != 'timestamp':
                    df = df.drop(columns=[timestamp_column])
            else:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á timestamp ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
                self.logger.warning("No timestamp column found, creating artificial timestamps")
                df['timestamp'] = pd.date_range(
                    start=datetime.now() - timedelta(days=len(df)), 
                    periods=len(df), 
                    freq='D'
                )
            
            return df
            
        except Exception as e:
            self.logger.error(f"Error processing timestamp: {e}")
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á timestamp ‡πÅ‡∏ö‡∏ö fallback
            df['timestamp'] = pd.date_range(
                start=datetime.now() - timedelta(days=len(df)), 
                periods=len(df), 
                freq='D'
            )
            return df
    
    def _process_ohlcv_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå OHLCV"""
        try:
            required_columns = ['open', 'high', 'low', 'close', 'volume']
            
            for col in required_columns:
                if col not in df.columns:
                    if col in ['open', 'high', 'low'] and 'close' in df.columns:
                        # ‡πÉ‡∏ä‡πâ close ‡πÅ‡∏ó‡∏ô‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏≠‡∏∑‡πà‡∏ô
                        df[col] = df['close']
                        self.logger.warning(f"Missing {col}, using close price")
                    elif col == 'volume':
                        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ volume ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                        df[col] = 1000000
                        self.logger.warning(f"Missing {col}, using default value")
                    else:
                        raise ValueError(f"Missing required column: {col}")
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                if df[col].isna().any():
                    # ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
                    df[col] = df[col].fillna(method='ffill').fillna(method='bfill').fillna(df[col].mean())
                    
                    if df[col].isna().any():
                        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏°‡∏µ NaN ‡∏≠‡∏¢‡∏π‡πà ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                        default_values = {
                            'open': 100, 'high': 100, 'low': 100, 'close': 100, 'volume': 1000000
                        }
                        df[col] = df[col].fillna(default_values[col])
            
            return df
            
        except Exception as e:
            self.logger.error(f"Error processing OHLCV columns: {e}")
            raise
    
    def _validate_ohlc_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• OHLC"""
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ High >= max(Open, Close) ‡πÅ‡∏•‡∏∞ Low <= min(Open, Close)
            df['high'] = np.maximum(df['high'], df[['open', 'close']].max(axis=1))
            df['low'] = np.minimum(df['low'], df[['open', 'close']].min(axis=1))
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ß‡∏Å
            price_columns = ['open', 'high', 'low', 'close']
            for col in price_columns:
                # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≤‡∏•‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
                invalid_mask = (df[col] <= 0)
                if invalid_mask.any():
                    mean_price = df[col][df[col] > 0].mean()
                    df.loc[invalid_mask, col] = mean_price
                    self.logger.warning(f"Fixed {invalid_mask.sum()} invalid {col} values")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö volume
            df['volume'] = np.maximum(df['volume'], 1)  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô volume ‡πÄ‡∏õ‡πá‡∏ô‡∏®‡∏π‡∏ô‡∏¢‡πå
            
            return df
            
        except Exception as e:
            self.logger.error(f"Error validating OHLC data: {e}")
            return df
    
    def get_combined_data(self, symbols: List[str], data_dir: str = "data") -> Dict[str, pd.DataFrame]:
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢ symbols (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á)"""
        try:
            combined_data = {}
            
            for symbol in symbols:
                self.logger.info(f"Loading data for {symbol}...")
                
                # ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏Å‡πà‡∏≠‡∏ô
                historical_data = self.load_historical_data(symbol, data_dir)
                
                if historical_data is not None and len(historical_data) > 50:
                    combined_data[symbol] = historical_data
                    self.logger.success(f"Using real data for {symbol}: {len(historical_data)} rows")
                else:
                    # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠
                    self.logger.warning(f"Using synthetic data for {symbol}")
                    synthetic_data = self.generate_synthetic_data(symbol, 1000)
                    combined_data[symbol] = synthetic_data
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ
            total_real_data = sum(1 for symbol, data in combined_data.items() 
                                if len(data) > 500)  # ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 500 ‡πÅ‡∏ñ‡∏ß
            
            self.logger.info(f"Data summary: {len(combined_data)} symbols loaded")
            self.logger.info(f"Real data: {total_real_data}, Synthetic data: {len(combined_data) - total_real_data}")
            
            return combined_data
            
        except Exception as e:
            self.logger.error(f"Error getting combined data: {e}")
            return {}
    
    def generate_synthetic_data(self, symbol: str, num_samples: int = 1000) -> pd.DataFrame:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô)"""
        try:
            # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ random seed ‡∏ï‡∏≤‡∏° symbol
            np.random.seed(hash(symbol) % 2**32)
            
            # ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á
            base_prices = {
                'BTC': 45000, 'ETH': 2800, 'BNB': 320, 'LTC': 95,
                'ADA': 0.5, 'DOT': 7, 'LINK': 15, 'UNI': 8
            }
            start_price = base_prices.get(symbol, 100)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
            timestamps = pd.date_range(
                start=datetime.now() - timedelta(days=num_samples),
                periods=num_samples,
                freq='1H'
            )
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏î‡πâ‡∏ß‡∏¢ sophisticated model
            prices = self._generate_realistic_prices(start_price, num_samples, symbol)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á OHLCV ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á
            data = self._generate_realistic_ohlcv(prices, timestamps, symbol)
            
            df = pd.DataFrame(data)
            
            # ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
            df = self._post_process_synthetic_data(df)
            
            self.logger.debug(f"Generated synthetic data for {symbol}: {len(df)} rows")
            return df
            
        except Exception as e:
            self.logger.error(f"Error generating synthetic data for {symbol}: {e}")
            raise
    
    def _generate_realistic_prices(self, start_price: float, num_samples: int, symbol: str) -> np.ndarray:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô"""
        prices = [start_price]
        
        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏° symbol
        volatility_params = {
            'BTC': {'base_vol': 0.035, 'trend_strength': 0.003},
            'ETH': {'base_vol': 0.045, 'trend_strength': 0.004},
            'BNB': {'base_vol': 0.040, 'trend_strength': 0.002},
            'LTC': {'base_vol': 0.050, 'trend_strength': 0.002}
        }
        
        params = volatility_params.get(symbol, {'base_vol': 0.030, 'trend_strength': 0.002})
        
        for i in range(1, num_samples):
            # Multiple time series components
            
            # 1. Long-term trend
            trend = np.sin(i * 0.005) * params['trend_strength']
            
            # 2. Medium-term cycles
            cycle = np.sin(i * 0.02) * 0.001 + np.cos(i * 0.03) * 0.0005
            
            # 3. Mean reversion
            deviation = (prices[-1] - start_price) / start_price
            mean_reversion = -0.05 * deviation if abs(deviation) > 0.1 else 0
            
            # 4. Volatility clustering (GARCH-like)
            if i > 1:
                prev_return = (prices[-1] - prices[-2]) / prices[-2]
                vol_clustering = 0.3 * abs(prev_return)
            else:
                vol_clustering = 0
            
            # 5. Random component
            base_volatility = params['base_vol'] * (1 + vol_clustering)
            random_component = np.random.normal(0, base_volatility)
            
            # 6. Jump component (rare large moves)
            if np.random.random() < 0.005:  # 0.5% chance of jump
                jump = np.random.normal(0, 0.1) * (1 if np.random.random() > 0.5 else -1)
            else:
                jump = 0
            
            # Combine all components
            total_change = trend + cycle + mean_reversion + random_component + jump
            
            # Apply change
            new_price = prices[-1] * (1 + total_change)
            
            # Ensure price stays positive and reasonable
            new_price = max(new_price, start_price * 0.1)  # Don't go below 10% of start price
            new_price = min(new_price, start_price * 10)   # Don't go above 10x start price
            
            prices.append(new_price)
        
        return np.array(prices)
    
    def _generate_realistic_ohlcv(self, prices: np.ndarray, timestamps: pd.DatetimeIndex, symbol: str) -> Dict:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á OHLCV ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á"""
        num_samples = len(prices)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á OHLC
        opens = np.zeros(num_samples)
        highs = np.zeros(num_samples)
        lows = np.zeros(num_samples)
        closes = prices.copy()
        
        # Volume parameters ‡∏ï‡∏≤‡∏° symbol
        volume_params = {
            'BTC': {'base': 20000000, 'multiplier': 0.5},
            'ETH': {'base': 15000000, 'multiplier': 0.6},
            'BNB': {'base': 5000000, 'multiplier': 0.7},
            'LTC': {'base': 3000000, 'multiplier': 0.8}
        }
        
        vol_param = volume_params.get(symbol, {'base': 1000000, 'multiplier': 1.0})
        
        for i in range(num_samples):
            if i == 0:
                opens[i] = closes[i]
            else:
                # Open ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ö close ‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô
                gap = np.random.normal(0, 0.002)  # Gap ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
                opens[i] = closes[i-1] * (1 + gap)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á intraday range
            volatility = abs(closes[i] - opens[i]) / opens[i] + 0.005  # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥
            
            # High ‡πÅ‡∏•‡∏∞ Low
            high_factor = 1 + abs(np.random.normal(0, volatility))
            low_factor = 1 - abs(np.random.normal(0, volatility))
            
            highs[i] = max(opens[i], closes[i]) * high_factor
            lows[i] = min(opens[i], closes[i]) * low_factor
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•
            highs[i] = max(highs[i], max(opens[i], closes[i]))
            lows[i] = min(lows[i], min(opens[i], closes[i]))
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Volume ‡∏ó‡∏µ‡πà‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•
        volumes = []
        for i in range(num_samples):
            # Volume ‡∏™‡∏π‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß‡∏°‡∏≤‡∏Å
            price_change = abs(closes[i] - opens[i]) / opens[i] if opens[i] > 0 else 0
            volume_multiplier = 1 + (price_change * 3)  # Volume ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß
            
            # Add some randomness
            random_factor = np.random.lognormal(0, vol_param['multiplier'])
            
            volume = vol_param['base'] * volume_multiplier * random_factor
            volumes.append(max(volume, vol_param['base'] * 0.1))  # Volume ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥
        
        return {
            'timestamp': timestamps,
            'open': opens,
            'high': highs,
            'low': lows,
            'close': closes,
            'volume': volumes,
            'symbol': symbol
        }
    
    def _post_process_synthetic_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á"""
        try:
            # ‡∏õ‡∏±‡∏î‡∏£‡∏≤‡∏Ñ‡∏≤‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á
            price_columns = ['open', 'high', 'low', 'close']
            for col in price_columns:
                if df[col].iloc[0] > 1000:  # ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏™‡∏π‡∏á ‡πÄ‡∏ä‡πà‡∏ô BTC
                    df[col] = np.round(df[col], 2)
                elif df[col].iloc[0] > 100:  # ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
                    df[col] = np.round(df[col], 3)
                else:  # ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πà‡∏≥
                    df[col] = np.round(df[col], 4)
            
            # ‡∏õ‡∏±‡∏î Volume
            df['volume'] = np.round(df['volume']).astype(int)
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° noise ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á
            for col in price_columns:
                noise = np.random.normal(0, df[col] * 0.0001, len(df))
                df[col] += noise
                df[col] = np.maximum(df[col], 0.01)  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡∏¥‡∏î‡∏•‡∏ö
            
            return df
            
        except Exception as e:
            self.logger.error(f"Error post-processing synthetic data: {e}")
            return df
    
    def create_features_from_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á technical indicators ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô)"""
        try:
            features_df = df.copy()
            
            # ‡∏î‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ indicators
            indicators_config = self.config.get_indicators_config()
            
            # Basic price data
            close = features_df['close']
            high = features_df['high']
            low = features_df['low']
            volume = features_df['volume']
            
            # 1. Moving Averages
            for period in indicators_config['moving_averages']:
                sma_col = f'sma_{period}'
                ema_col = f'ema_{period}'
                
                # Simple Moving Average
                features_df[sma_col] = close.rolling(window=period, min_periods=1).mean()
                
                # Exponential Moving Average
                features_df[ema_col] = close.ewm(span=period).mean()
                
                # Price to MA ratios
                features_df[f'price_to_{sma_col}'] = close / features_df[sma_col]
                features_df[f'price_to_{ema_col}'] = close / features_df[ema_col]
            
            # 2. RSI
            rsi_period = indicators_config['rsi_period']
            delta = close.diff()
            gain = delta.where(delta > 0, 0).rolling(window=rsi_period, min_periods=1).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=rsi_period, min_periods=1).mean()
            rs = gain / (loss + 1e-8)
            features_df['rsi'] = 100 - (100 / (1 + rs))
            
            # 3. MACD
            fast_period = indicators_config['macd_fast']
            slow_period = indicators_config['macd_slow']
            signal_period = indicators_config['macd_signal']
            
            ema_fast = close.ewm(span=fast_period).mean()
            ema_slow = close.ewm(span=slow_period).mean()
            features_df['macd'] = ema_fast - ema_slow
            features_df['macd_signal'] = features_df['macd'].ewm(span=signal_period).mean()
            features_df['macd_histogram'] = features_df['macd'] - features_df['macd_signal']
            
            # 4. Bollinger Bands
            bb_period = indicators_config['bollinger_period']
            bb_std = indicators_config['bollinger_std']
            
            bb_sma = close.rolling(window=bb_period, min_periods=1).mean()
            bb_std_dev = close.rolling(window=bb_period, min_periods=1).std()
            features_df['bb_upper'] = bb_sma + (bb_std_dev * bb_std)
            features_df['bb_lower'] = bb_sma - (bb_std_dev * bb_std)
            features_df['bb_position'] = (close - features_df['bb_lower']) / (features_df['bb_upper'] - features_df['bb_lower'] + 1e-8)
            features_df['bb_width'] = (features_df['bb_upper'] - features_df['bb_lower']) / bb_sma
            
            # 5. Volatility measures
            returns = close.pct_change().fillna(0)
            for window in indicators_config['volatility_windows']:
                features_df[f'volatility_{window}'] = returns.rolling(window=window, min_periods=1).std()
                features_df[f'volatility_{window}_annualized'] = features_df[f'volatility_{window}'] * np.sqrt(252)
            
            # 6. Momentum indicators
            for period in indicators_config['momentum_periods']:
                features_df[f'momentum_{period}'] = close.pct_change(periods=period).fillna(0)
                features_df[f'roc_{period}'] = ((close - close.shift(period)) / close.shift(period)).fillna(0)
            
            # 7. Volume indicators
            volume_sma_20 = volume.rolling(window=20, min_periods=1).mean()
            features_df['volume_ratio'] = volume / (volume_sma_20 + 1e-8)
            features_df['volume_sma_ratio'] = volume_sma_20 / volume.rolling(window=50, min_periods=1).mean()
            
            # OBV (On-Balance Volume)
            obv = [0]
            for i in range(1, len(close)):
                if close.iloc[i] > close.iloc[i-1]:
                    obv.append(obv[-1] + volume.iloc[i])
                elif close.iloc[i] < close.iloc[i-1]:
                    obv.append(obv[-1] - volume.iloc[i])
                else:
                    obv.append(obv[-1])
            features_df['obv'] = obv
            
            # 8. Price action indicators
            features_df['price_range'] = (high - low) / (close + 1e-8)
            features_df['body_size'] = abs(close - features_df['open']) / (close + 1e-8)
            features_df['upper_shadow'] = (high - np.maximum(close, features_df['open'])) / (close + 1e-8)
            features_df['lower_shadow'] = (np.minimum(close, features_df['open']) - low) / (close + 1e-8)
            
            # 9. Support and Resistance levels
            features_df['resistance_20'] = high.rolling(window=20, min_periods=1).max()
            features_df['support_20'] = low.rolling(window=20, min_periods=1).min()
            features_df['price_position'] = (close - features_df['support_20']) / (features_df['resistance_20'] - features_df['support_20'] + 1e-8)
            
            # 10. Trend indicators
            features_df['trend_5'] = (features_df['sma_5'] > features_df['sma_5'].shift(1)).astype(int)
            features_df['trend_20'] = (features_df['sma_20'] > features_df['sma_20'].shift(1)).astype(int)
            features_df['ma_cross'] = (features_df['sma_5'] > features_df['sma_20']).astype(int)
            
            # Fill NaN values
            features_df = features_df.fillna(method='ffill').fillna(method='bfill').fillna(0)
            
            # Remove infinite values
            features_df = features_df.replace([np.inf, -np.inf], np.nan)
            features_df = features_df.fillna(0)
            
            self.logger.debug(f"Created {len(features_df.columns)} features")
            return features_df
            
        except Exception as e:
            self.logger.error(f"Error creating features: {e}")
            return df
    
    def get_data_summary(self, symbols: List[str], data_dir: str = "data") -> Dict[str, Any]:
        """‡∏î‡∏∂‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        try:
            summary = {
                'total_symbols': len(symbols),
                'available_files': [],
                'missing_files': [],
                'data_quality': {}
            }
            
            for symbol in symbols:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ
                possible_files = [
                    f"{symbol}_yfinance_daily.csv",
                    f"{symbol}_yfinance_daily_backup.csv"
                ]
                
                found_files = []
                for filename in possible_files:
                    filepath = os.path.join(data_dir, filename)
                    if os.path.exists(filepath):
                        found_files.append(filename)
                
                if found_files:
                    summary['available_files'].append({
                        'symbol': symbol,
                        'files': found_files
                    })
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                    try:
                        data = self.load_historical_data(symbol, data_dir)
                        if data is not None:
                            summary['data_quality'][symbol] = {
                                'rows': len(data),
                                'date_range': {
                                    'start': data['timestamp'].min().strftime('%Y-%m-%d'),
                                    'end': data['timestamp'].max().strftime('%Y-%m-%d')
                                },
                                'completeness': (1 - data.isna().sum().sum() / (len(data) * len(data.columns))) * 100
                            }
                    except Exception as e:
                        summary['data_quality'][symbol] = {'error': str(e)}
                else:
                    summary['missing_files'].append(symbol)
            
            return summary
            
        except Exception as e:
            self.logger.error(f"Error getting data summary: {e}")
            return {}

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
def check_data_availability(data_dir: str = "data") -> Dict[str, Any]:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    
    symbols = ['BTC', 'ETH', 'BNB', 'LTC']
    data_manager = DataManager()
    
    print("üîç Checking data availability...")
    print("=" * 50)
    
    summary = data_manager.get_data_summary(symbols, data_dir)
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
    print(f"üìä Total symbols: {summary['total_symbols']}")
    print(f"‚úÖ Available: {len(summary['available_files'])}")
    print(f"‚ùå Missing: {len(summary['missing_files'])}")
    print()
    
    # ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ
    if summary['available_files']:
        print("üìÅ Available files:")
        for item in summary['available_files']:
            print(f"  {item['symbol']}: {', '.join(item['files'])}")
        print()
    
    # ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
    if summary['missing_files']:
        print("üö´ Missing files:")
        for symbol in summary['missing_files']:
            print(f"  {symbol}: No data files found")
        print()
    
    # ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    if summary['data_quality']:
        print("üìà Data quality:")
        for symbol, quality in summary['data_quality'].items():
            if 'error' not in quality:
                print(f"  {symbol}: {quality['rows']} rows, "
                      f"{quality['date_range']['start']} to {quality['date_range']['end']}, "
                      f"Complete: {quality['completeness']:.1f}%")
            else:
                print(f"  {symbol}: Error - {quality['error']}")
    
    return summary

if __name__ == "__main__":
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö
    check_data_availability()